{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0701f0be",
   "metadata": {},
   "source": [
    "# Model validation using deep generation of stress data: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9e964",
   "metadata": {},
   "source": [
    "## Background: generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69620620",
   "metadata": {},
   "source": [
    "| Discriminative models | Generative models |\n",
    "|:---|:---|\n",
    "|<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/discriminative_learning.png' width=300>|<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/generative_learning.png' width=300>|\n",
    "| Aims to predict the label by the given example| Aims to learn the full data distribution by the given subsample |\n",
    "| Learns the probability $y \\sim P(y|X)$ | Learns the probability $X,y \\sim P(X, y)$ or $X \\sim P(X)$ |\n",
    "| _Examples of tasks:_ | _Examples of tasks:_ |\n",
    "| 1. Is a cat on the photo? | 1. Create a photo of a cat that looks like the average of two given cat photos |\n",
    "| 2. How old is a person on the photo? | 2. Create a photo of an avocado armchair |\n",
    "| 3. What is the most likely number of tourists the next mounth? | 3. Write an essay on the topic of global warming |\n",
    "| _Examples of models:_ | _Examples of models:_ |\n",
    "| Decision Tree, Support Vector Machine (SVM), Convolutional Neural Network (CNN) | Autoregressive model (AR), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), Generative Pretrained Transformer (GPT)|\n",
    "\n",
    "_An image is taken from https://medium.com/@jordi299/about-generative-and-discriminative-models-d8958b67ad32_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b375e44",
   "metadata": {},
   "source": [
    "Deep generative models are based on deep neural networks.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/space_generator.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057dc81",
   "metadata": {},
   "source": [
    "## Background: Generative Adversarial Network for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b58c31",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/gan_for_ts.png' width=600>\n",
    "\n",
    "A Generative Adversarial Network (GAN) is a type of model that consists of two neural networks: a generator and a discriminator. \n",
    "\n",
    "* The objective of a generator (G) is to create realistic data\n",
    "* The objective of a discriminator (D) is to detect fake data\n",
    "\n",
    "We note the multivariate time series as $X_1, X_2, \\dots, X_N$, where $X_t \\in R^d$\n",
    "\n",
    "The general objective of GAN is\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb E_{X} \\log(D(X)) + \\mathbb E_{Z} \\log(1 - D(G(Z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23320ce",
   "metadata": {},
   "source": [
    "In this tutorial we use GAN where a generator and a discriminator are Temporal Convolutional Networks (TCN).\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/tcn.png' width=600>\n",
    "\n",
    "_An image is taken from Oord, Aaron van den, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 (2016)._\n",
    "\n",
    "The main advantages of TCN:\n",
    "* Exponentially large receptive field — TCN can detect long and short patterns\n",
    "* Can be applied to time series of arbitrary length\n",
    "* Causal convolution prevents leakage from the future\n",
    "\n",
    "The key parameter of TCN is the number of channels (also known as filters, kernels).\n",
    "\n",
    "For generator:\n",
    "* Input channels: latent space dimensionality\n",
    "* Hidden channels: some large number to model temporal relationships\n",
    "* Output channels: time series dimensionality\n",
    "\n",
    "For discriminator:\n",
    "* Input channels: time series dimensionality\n",
    "* Hidden channels: some large number to model temporal relationships\n",
    "* Output channels: 1 (the probability that the data is fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39720e9e",
   "metadata": {},
   "source": [
    "## Generation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d53aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/airi-industrial-ai/ec23-tutorial -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7341460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "import requests\n",
    "\n",
    "from ectutorial.data import SlidingWindowDataset, get_ett_dataset\n",
    "from ectutorial.gen_models import TCNGANModule\n",
    "from ectutorial.plot import plot_pair_fake_val, plot_risk\n",
    "from ectutorial.wcr import calculate_worst_case_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cab082",
   "metadata": {},
   "source": [
    "We download the dataset of energy consumption that we used in the first part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b842bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ett = get_ett_dataset()\n",
    "ett.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65104b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in ett.columns:\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    ett[col].iloc[:500].plot()\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c04487",
   "metadata": {},
   "source": [
    "We split the dataset into train, validation, test sets.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/train_val_test_split.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4542ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_thresh = int(ett.shape[0] * 0.7)\n",
    "test_thresh = int(ett.shape[0] * 0.85)\n",
    "\n",
    "train_data = ett.iloc[:val_thresh].copy()\n",
    "val_data = ett.iloc[val_thresh:test_thresh].copy()\n",
    "test_data = ett.iloc[test_thresh:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde46a46",
   "metadata": {},
   "source": [
    "We apply a standard scaler for training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b18d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "val_data_norm = scaler.fit_transform(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede8deb",
   "metadata": {},
   "source": [
    "We define a sliding window dataset and data loader. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/sliding_window.png' width=800>\n",
    "\n",
    "The dataloader split the dataset into batches, where each batch consists of a few time series samples.\n",
    "\n",
    "We will use the __validation set for training GAN__, since we are looking for shifts with respect to the validation period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 200\n",
    "train_dataset = SlidingWindowDataset(val_data_norm, window_size=window_size, step_size=1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff3fbf",
   "metadata": {},
   "source": [
    "The shape of a batch is (the batch size, the length of time series, the dimensionality of time series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15031987",
   "metadata": {},
   "source": [
    "Let us define our TCN GAN:\n",
    "* `latent_dim` — the dimensionality of latent space which is Miltivariate Normal Distribution.\n",
    "* `hidden_dim` — the number of channels/filters/kernels in the hidden layers\n",
    "* `target_dim` — the dimenstionality of time series\n",
    "* `num_layers` — the number of TCN layers\n",
    "* `lr` — the learning rate for the optimization step\n",
    "\n",
    "The values of parameters can be tuned using some search strategies, such as grid or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = TCNGANModule(\n",
    "    latent_dim=8,\n",
    "    target_dim=len(ett.columns),\n",
    "    hidden_dim=64,\n",
    "    num_layers=4,\n",
    "    lr=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646ea00",
   "metadata": {},
   "source": [
    "GAN is known as an unstable model in training, since the generator and discriminator fluctuate without a guarantee of convergence. Thus, it is strongly recommended to monitor auxiliary metrics to determine the total number of training steps.\n",
    "\n",
    "In the case of generating time series, metrics can be:\n",
    "1. The average discrepancy between a minimum of real and fake time series\n",
    "2. The average discrepancy between a maximum of real and fake time series\n",
    "3. The average discrepancy between a mean of real and fake time series\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/notebooks/gan_training.png' width=600>\n",
    "\n",
    "In our tutorial, we skip the training part to save time. It takes about 10 minutes on NVIDIA V100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "trainer = Trainer(\n",
    "    accelerator='auto',\n",
    "    max_epochs=200,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=gan,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=train_loader,\n",
    ")\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f3948",
   "metadata": {},
   "source": [
    "We load the pretrained model from a `checkpoint` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/airi-industrial-ai/ec23-tutorial/main/models/ver=6-epoch=0-step=76.ckpt'\n",
    "r = requests.get(url)\n",
    "open('ver=6-epoch=0-step=76.ckpt', 'wb').write(r.content);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = TCNGANModule.load_from_checkpoint(\n",
    "    'ver=6-epoch=0-step=76.ckpt',\n",
    "    map_location='cpu',\n",
    "    latent_dim=8,\n",
    "    target_dim=len(ett.columns),\n",
    "    hidden_dim=64,\n",
    "    num_layers=4,\n",
    "    lr=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d700b",
   "metadata": {},
   "source": [
    "Let us plot the pair plot to compare the fake and validation data. We can make fake data more \"risky\" by increasing the variance of the latent space using the temp parameter.\n",
    "\n",
    "As our generator is a convolutional network, we can create a sequnce of an arbitrary length, for example 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d62386",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "torch.manual_seed(0)\n",
    "fake = gan.sample(seq_len, temp=1)\n",
    "\n",
    "plot_pair_fake_val(fake, val_data, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf574f8",
   "metadata": {},
   "source": [
    "Let's also plot the trajectories of fake time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f702c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fakes = []\n",
    "for _ in range(5):\n",
    "    fake = gan.sample(len(val_data), temp=1.)\n",
    "    fake = scaler.inverse_transform(fake)\n",
    "    fake_data = pd.DataFrame(\n",
    "        fake,\n",
    "        index=val_data.index,\n",
    "        columns=val_data.columns\n",
    "    )\n",
    "    fakes.append(fake_data)\n",
    "for col in fake_data.columns:\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.plot(val_data[col].values[:500], label='val data', c='black')\n",
    "    for fake_data in fakes:\n",
    "        plt.plot(fake_data[col].values[:500], alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c9601",
   "metadata": {},
   "source": [
    "We can see that the fake time series are similar to real ones. There are also some metrics such as ACF error that can give us numerical estimation of closeness to real time series. For more information, see, for example, Wiese, Magnus, et al. \"Quant GANs: deep generation of financial time series.\" Quantitative Finance 20.9 (2020): 1419-1440.\n",
    "\n",
    "Conclusions so far:\n",
    "* We split the dataset into train, validation, test sets\n",
    "* We trained GAN on the validation set, as we are looking for shifts close to the validation period\n",
    "* We created fake time series and visually checked that they are similar to the real ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682384d",
   "metadata": {},
   "source": [
    "## Worst case risk on fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b834b",
   "metadata": {},
   "source": [
    "Now we are ready to compare risk estimation on validation and fake sets. We download the dataset and prepare it for the regression task — predict OT attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d39c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_ett_dataset()\n",
    "data[\"dom\"] = data.index.day\n",
    "data[\"dow\"] = data.index.weekday\n",
    "data[\"hour\"] = data.index.hour\n",
    "data[\"mnth\"] = data.index.month\n",
    "data['OT-1'] = data.OT.shift(1)\n",
    "data = data.iloc[1:]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad77b0",
   "metadata": {},
   "source": [
    "Let us create the fake time series using GAN and then prepare the dataset in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "seq_len = len(val_data) * 10\n",
    "fake = gan.sample(seq_len, temp=1)\n",
    "fake = scaler.inverse_transform(fake)\n",
    "fake_data = pd.DataFrame(\n",
    "    fake,\n",
    "    index=pd.date_range(start=val_data.index[0], periods=seq_len, freq='1H'),\n",
    "    columns=val_data.columns\n",
    ")\n",
    "fake_data[\"dom\"] = fake_data.index.day\n",
    "fake_data[\"dow\"] = fake_data.index.weekday\n",
    "fake_data[\"hour\"] = fake_data.index.hour\n",
    "fake_data[\"mnth\"] = fake_data.index.month\n",
    "fake_data['OT-1'] = fake_data.OT.shift(1)\n",
    "fake_data = fake_data.iloc[1:]\n",
    "fake_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d35da",
   "metadata": {},
   "source": [
    "Next, we prepare train, validation, test and fake sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac853c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'OT'\n",
    "feat_cols = list(set(data.columns).difference([target_col]))\n",
    "\n",
    "X = data[feat_cols]\n",
    "y = data[target_col]\n",
    "\n",
    "X_train = X.iloc[:val_thresh]\n",
    "y_train = y.iloc[:val_thresh]\n",
    "\n",
    "X_val = X.iloc[val_thresh:test_thresh]\n",
    "y_val = y.iloc[val_thresh:test_thresh]\n",
    "\n",
    "X_test = X.iloc[test_thresh:]\n",
    "y_test = y.iloc[test_thresh:]\n",
    "\n",
    "X_fake = fake_data[feat_cols]\n",
    "y_fake = fake_data[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bd0f7",
   "metadata": {},
   "source": [
    "Train a linear regression model and calculate the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ffa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "preds_val = lr.predict(X_val)\n",
    "preds_test = lr.predict(X_test)\n",
    "preds_fake = lr.predict(X_fake)\n",
    "\n",
    "err_val = (y_val - preds_val) ** 2\n",
    "err_test = (y_test - preds_test) ** 2\n",
    "err_fake = (y_fake - preds_fake) ** 2\n",
    "\n",
    "print(f'RMSE on validation: {np.sqrt(err_val.mean()):.4f}')\n",
    "print(f'RMSE on fake: {np.sqrt(err_fake.mean()):.4f}')\n",
    "print(f'RMSE on test: {np.sqrt(err_test.mean()):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bda12c",
   "metadata": {},
   "source": [
    "As we see, the quality on the validation set is close to the fake set. It means that the model cannot detect much discrepancy between them.\n",
    "\n",
    "We define mutable and immutable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd953223",
   "metadata": {},
   "outputs": [],
   "source": [
    "immutable = ['hour', 'mnth', 'dow', 'dom']\n",
    "mutable = ['MUFL', 'HUFL', 'HULL', 'OT-1', 'MULL', 'LULL', 'LUFL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13dba8",
   "metadata": {},
   "source": [
    "We calculate worst case risk on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_log, lb_log, ub_log = calculate_worst_case_risk(err_val, X_val, immutable, mutable)\n",
    "plot_risk(mn_log, lb_log, ub_log, err_test, 'Worst case risk on validation set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480d64c",
   "metadata": {},
   "source": [
    "And then: Worst case risk on fake data vs loss on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_log, lb_log, ub_log = calculate_worst_case_risk(err_fake, X_fake, immutable, mutable)\n",
    "plot_risk(mn_log, lb_log, ub_log, err_test, 'Worst case risk on fake set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9fff8",
   "metadata": {},
   "source": [
    "As we can see, we can get a narrower uncertainty interval for fake data. \n",
    "\n",
    "This means that we can more precisely assess the robustness of the model using fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891c090",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878e47f",
   "metadata": {},
   "source": [
    "Generative models produce realistic data that can help estimate the model robustness to dataset shifts, however it brings some disadvantages.\n",
    "\n",
    "1. It is hard to tune parameters and train deep generative models such GAN\n",
    "2. It is hard to estimate how close the fake time series to real ones: no scientific standard so far\n",
    "3. We assume that generative model can produce original data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f62c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In our two parts of tutorial on model validation using deep generation of stress data:\n",
    "\n",
    "Part 1.\n",
    "\n",
    "1. We considered the concept of supervised learning and the problem of overfitting\n",
    "2. We considered the dataset shift problem, the definition and examples in real life\n",
    "3. We downloaded data that contains the time series with energy consumption\n",
    "4. We split the data into train, validation and test periods and observed shifts\n",
    "5. We trained the regression model to forecast the oil temperature \n",
    "6. We investigated the quality reduction in the test set\n",
    "7. We evaluated the model robustness to dataset shifts using the Worst Case Risk methodology\n",
    "\n",
    "Part 2.\n",
    "\n",
    "8. We considered the concept of generative learning\n",
    "9. We created the GAN model based on TCN to generate time series\n",
    "10. We trained the GAN using validation data and generated fake time series\n",
    "11. We visually evaluated the quality of fake time series\n",
    "12. We evaluated the model robustness using fake data and got a more accurate estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3aee0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
